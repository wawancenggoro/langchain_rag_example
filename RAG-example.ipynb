{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "62d4b880-4f71-475d-968d-a605d5380582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6108dc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('var.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fd5fb4-e2ad-4c37-ac29-b4b65dbf8d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"qwen/qwen25-72b-instruct\",\n",
    "    temperature=1,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    base_url=\"https://dekallm.cloudeka.ai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c592766-dace-4d69-af6b-6a777c961342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5559a2ce-9735-4f65-8482-d289a663578b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything specific.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17eeed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = './cendol.pdf'\n",
    "loader = PyPDFLoader(file_path=pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
    "split_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04a68602",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"baai/bge-multilingual-gemma2\",\n",
    "    base_url=\"https://dekallm.cloudeka.ai/\"\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(split_documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "914e6eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "Instruct (Cendolinst) and the second phase models\n",
      "as Cendol-Chat (Cendolchat). We report the com-\n",
      "plete hyperparameters used in Appendix A.\n",
      "====================================================\n",
      "prompts, i.e., identity prompt, safety prompt, and\n",
      "computational creativity prompt.\n",
      "Identity Prompt Identity prompts are incorpo-\n",
      "rated to provide a faithful identity of the Cendol\n",
      "models. These identity prompts include the per-\n",
      "sonal identity of Cendol, the etymology of the word\n",
      "“cendol”, the creator information of Cendol, and\n",
      "the neutrality of Cendol on various aspects, e.g.,\n",
      "gender, religion, and political stance. In addition,\n",
      "we also include some trivia prompts to increase the\n",
      "engagingness of using Cendol. In total, we cover\n",
      "125 identity prompts and to increase the representa-\n",
      "tion of these prompts, we upsample the number of\n",
      "identity prompts by 500 in the Cendol Collection.\n",
      "Safety Prompt We manually construct safety\n",
      "prompts to prevent Cendol from responding to\n",
      "queries that are not appropriate according to cul-\n",
      "tural norms and values in Indonesia. The safety\n",
      "prompts include prompts for guard-railing illegal\n",
      "activities, e.g., prostitution, gambling, illegal drugs,\n",
      "====================================================\n",
      "LLMs hurts their ability to generate responses in\n",
      "Indonesian and other underrepresented languages.\n",
      "This also leads to inefficiency during inference\n",
      "due to the vocabulary mismatch, hence texts in\n",
      "these languages are tokenized into much longer\n",
      "tokens (Ahia et al., 2023). Additionally, these\n",
      "LLMs are more prone to safety issues, e.g., giving\n",
      "unsafe responses (Wang et al., 2023b), hallucina-\n",
      "tions (Guerreiro et al., 2023; Bang et al., 2023), and\n",
      "jailbreaking (Yong et al., 2023; Deng et al., 2023).\n",
      "To overcome the challenge of weak language\n",
      "representation in Indonesian languages, we intro-\n",
      "duce Cendol2, a series of large-scale instruction-\n",
      "2Cendol is an iced sweet dessert that contains droplets of\n",
      "pandan-flavored green rice flour jelly and coconut milk, served\n",
      "with palm sugar syrup. Cendol is popular across Southeast\n",
      "arXiv:2404.06138v2  [cs.CL]  8 Jul 2024\n"
     ]
    }
   ],
   "source": [
    "# Use the vectorstore as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Retrieve the most similar text\n",
    "retrieved_documents = retriever.invoke(\"What is cendol?\")\n",
    "\n",
    "# show the retrieved document's content]\n",
    "for i in range(3):\n",
    "    print('====================================================')\n",
    "    print(retrieved_documents[i].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vector store\n",
    "vectorstore.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b97cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vector store\n",
    "new_vectorstore = FAISS.load_local(\n",
    "       \"faiss_index\", embeddings, allow_dangerous_deserialization=True\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddb5eba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wawan/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/client.py:277: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "       llm, retrieval_qa_chat_prompt\n",
    "   )\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "       new_vectorstore.as_retriever(), combine_docs_chain\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad26536f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fda250377f0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7fda5f079ab0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'retrieval-qa-chat', 'lc_hub_commit_hash': 'b60afb6297176b022244feb83066e10ecadcda7b90423654c4a9d45e7a73cebc'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer any use questions based solely on the context below:\\n\\n<context>\\n{context}\\n</context>'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fda5c0f25c0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fda5c10cee0>, root_client=<openai.OpenAI object at 0x7fda5c05cdc0>, root_async_client=<openai.AsyncOpenAI object at 0x7fda5c0f2620>, model_name='qwen/qwen25-72b-instruct', temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://dekallm.cloudeka.ai/', max_retries=2)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c585e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7fda5f079ab0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'retrieval-qa-chat', 'lc_hub_commit_hash': 'b60afb6297176b022244feb83066e10ecadcda7b90423654c4a9d45e7a73cebc'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer any use questions based solely on the context below:\\n\\n<context>\\n{context}\\n</context>'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fda5c0f25c0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fda5c10cee0>, root_client=<openai.OpenAI object at 0x7fda5c05cdc0>, root_async_client=<openai.AsyncOpenAI object at 0x7fda5c0f2620>, model_name='qwen/qwen25-72b-instruct', temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://dekallm.cloudeka.ai/', max_retries=2)\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_docs_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6cd9c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cendol is an iced sweet dessert that contains droplets of pandan-flavored green rice flour jelly and coconut milk, served with palm sugar syrup. It is a popular dessert across Southeast Asia.\n"
     ]
    }
   ],
   "source": [
    "res = retrieval_chain.invoke({\"input\": \"What is Cendol?\"})\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6531e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def load_prompt():\n",
    "        prompt = \"\"\" You need to answer the question in the sentence as same as in the  pdf content. . \n",
    "        Given below is the context and question of the user.\n",
    "        context = {context}\n",
    "        question = {question}\n",
    "        if the answer is not in the pdf , answer \"i donot know what the hell you are asking about\"\n",
    "         \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(prompt)\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4b7de05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt=load_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a95f9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08129a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cendol is an iced sweet dessert that contains droplets of pandan-flavored green rice flour jelly and coconut milk, served with palm sugar syrup. Cendol is popular across Southeast Asia.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke('What is Cendol?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12ea5f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You need to answer the question in the sentence as same as in the  pdf content. . \n",
      "        Given below is the context and question of the user.\n",
      "        context = {context}\n",
      "        question = {question}\n",
      "        if the answer is not in the pdf , answer \"i donot know what the hell you are asking about\"\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.get_prompts()[0].messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de5406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
