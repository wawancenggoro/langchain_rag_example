{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d4b880-4f71-475d-968d-a605d5380582",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6108dc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv('var.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0fd5fb4-e2ad-4c37-ac29-b4b65dbf8d5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"qwen/qwen25-72b-instruct\",\n",
    "    temperature=1,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    base_url=\"https://dekallm.cloudeka.ai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c592766-dace-4d69-af6b-6a777c961342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5559a2ce-9735-4f65-8482-d289a663578b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello! It's nice to meet you. How can I assist you today? Whether you have questions, need information, or just want to chat, I'm here to help!\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17eeed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = './cloudeka.pdf'\n",
    "loader = PyPDFLoader(file_path=pdf_path)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
    "split_documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a68602",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"baai/bge-multilingual-gemma2\",\n",
    "    base_url=\"https://dekallm.cloudeka.ai/\"\n",
    ")\n",
    "\n",
    "vectorstore = FAISS.from_documents(split_documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "914e6eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "1\n",
      "Service Portal Cloudeka\n",
      "Cloudeka is a Cloud Computing platform that provides various cloud services\n",
      "including computing, storage, networking, and more. Cloudeka is supported\n",
      "by self-service through the dashboard service portal with features to\n",
      "configure, create projects, check billing, view and create rules in the\n",
      "organization. users can choose carefully from these services to develop new\n",
      "applications, or run existing applications on Cloudeka. Users can choose\n",
      "carefully the services to develop new applications, or to run existing\n",
      "applications on Cloudeka. There are two types of projects: Prepaid and \n",
      "Postpaid.\n",
      "1.Prepaid\n",
      "For the Prepaid type, it is used for personal needs that use personal email\n",
      "addresses and can only have one project so that the subscription period is\n",
      "relatively short without a letter of contract. For payment methods using \n",
      "Virtual Account, LinkAja, OVO, Credit Card.\n",
      "At the beginning of registration, the deposit must be at least IDR 50,000.00.\n",
      "====================================================\n",
      "Make sure that the deposit is enough to be able to use a new service\n",
      "subscription, if the daily requirement used is above IDR.50,000,00, then at\n",
      "least the deposit content is as big as the daily requirement. This type is\n",
      "calculated using three methods: PPU (Pay Per Use), Monthly, and One Time.\n",
      "2.Postpaid\n",
      "C lo u d e k a\n",
      "3/27/25, 6:44 AM Service Portal Cloudeka | Cloudeka\n",
      "====================================================\n",
      "2\n",
      "The Postpaid type is used for companies and registers using the company's\n",
      "email address so that the subscription period is relatively long and there is a\n",
      "contract letter. For payment methods using Virtual Account and Credit Card.\n",
      "Next\n",
      "Starter Guide Deka Flexi\n",
      "Last updated 8 months ago\n",
      "3/27/25, 6:44 AM Service Portal Cloudeka | Cloudeka\n"
     ]
    }
   ],
   "source": [
    "# Use the vectorstore as a retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Retrieve the most similar text\n",
    "retrieved_documents = retriever.invoke(\"What is cloudeka?\")\n",
    "\n",
    "# show the retrieved document's content]\n",
    "for i in range(3):\n",
    "    print('====================================================')\n",
    "    print(retrieved_documents[i].page_content[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "688f283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vector store\n",
    "vectorstore.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b97cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vector store\n",
    "new_vectorstore = FAISS.load_local(\n",
    "       \"faiss_index\", embeddings, allow_dangerous_deserialization=True\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddb5eba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wawan/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/client.py:277: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "       llm, retrieval_qa_chat_prompt\n",
    "   )\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(\n",
    "       new_vectorstore.as_retriever(), combine_docs_chain\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad26536f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fce00d5f1c0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7fce39f75ab0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'retrieval-qa-chat', 'lc_hub_commit_hash': 'b60afb6297176b022244feb83066e10ecadcda7b90423654c4a9d45e7a73cebc'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer any use questions based solely on the context below:\\n\\n<context>\\n{context}\\n</context>'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fce00d33070>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fce00d5d9c0>, root_client=<openai.OpenAI object at 0x7fce3befd870>, root_async_client=<openai.AsyncOpenAI object at 0x7fce00d330d0>, model_name='qwen/qwen25-72b-instruct', temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://dekallm.cloudeka.ai/', max_retries=2)\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c585e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7fce39f75ab0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'retrieval-qa-chat', 'lc_hub_commit_hash': 'b60afb6297176b022244feb83066e10ecadcda7b90423654c4a9d45e7a73cebc'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer any use questions based solely on the context below:\\n\\n<context>\\n{context}\\n</context>'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7fce00d33070>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7fce00d5d9c0>, root_client=<openai.OpenAI object at 0x7fce3befd870>, root_async_client=<openai.AsyncOpenAI object at 0x7fce00d330d0>, model_name='qwen/qwen25-72b-instruct', temperature=1.0, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://dekallm.cloudeka.ai/', max_retries=2)\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_docs_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6cd9c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloudeka is a Cloud Computing platform that offers a variety of cloud services, including computing, storage, networking, and more. It supports self-service through a dashboard service portal, where users can configure, create projects, check billing, and manage rules within the organization. Users can leverage these services to develop new applications or run existing ones on the Cloudeka platform.\n"
     ]
    }
   ],
   "source": [
    "res = retrieval_chain.invoke({\"input\": \"What is Cloudeka?\"})\n",
    "print(res[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6531e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Dependencies\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def load_prompt():\n",
    "        prompt = \"\"\" You need to answer the question in the sentence as same as in the  pdf content. . \n",
    "        Given below is the context and question of the user.\n",
    "        context = {context}\n",
    "        question = {question}\n",
    "        if the answer is not in the pdf , answer \"i donot know what the hell you are asking about\"\n",
    "         \"\"\"\n",
    "        prompt = ChatPromptTemplate.from_template(prompt)\n",
    "        return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b7de05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "prompt=load_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a95f9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08129a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cendol is an iced sweet dessert that contains droplets of pandan-flavored green rice flour jelly and coconut milk, served with palm sugar syrup. Cendol is popular across Southeast Asia.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke('What is cloudeka?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12ea5f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " You need to answer the question in the sentence as same as in the  pdf content. . \n",
      "        Given below is the context and question of the user.\n",
      "        context = {context}\n",
      "        question = {question}\n",
      "        if the answer is not in the pdf , answer \"i donot know what the hell you are asking about\"\n",
      "         \n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.get_prompts()[0].messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75de5406",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
